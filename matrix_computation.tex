
\documentclass{beamer}

%\usepackage{listings}
%\usepackage[francais]{babel}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
%\usepackage{MyriadPro}
\usepackage{cabin}
\usepackage{graphicx}
\usepackage{array}
\usepackage{tikz}
\usetikzlibrary{positioning, backgrounds, shapes, chains, decorations.pathmorphing}

\usepackage{amsmath,amsthm,amssymb}  
\usepackage{stmaryrd}
%\usepackage{mdsymbol}
\usepackage{MnSymbol}
\usepackage{xcolor}
\usepackage{verbatim}
\usepackage{array}
\usepackage{multirow}
%\usepackage{csquotes}



\usepackage[absolute,overlay]{textpos}
%\usepackage[texcoord,
%grid,gridcolor=red!10,subgridcolor=green!10,gridunit=pt]
%{eso-pic}



\useoutertheme{infolines}

\newcommand{\hidden}[1]{}

%colors
\definecolor{darkgreen}{rgb}{0,0.5,0}
\usebeamercolor{block title}
\definecolor{beamerblue}{named}{fg}
\usebeamercolor{alert block title}
\definecolor{beamealert}{named}{fg}

\renewcommand{\colon}{\!:\!}


\newcommand\paraitem{%
 \quad
 \makebox[\labelwidth][r]{%
 \makelabel{%
 \usebeamertemplate{itemize \beameritemnestingprefix item}}}\hskip\labelsep}

\newcommand{\mmid}{\mathbin{{\mid}{\mid}}}

\begin{document}

\title{Further computations with matrices.} 
\author{Antoine Venant}
%\institute{UDS COLI}
\date{\today}

\maketitle

\begin{frame}{Invertible matrix.}

  \begin{block}{Definition}
    An $(n,n)$ (square) matrix $M$ is \emph{invertible} if there exists a matrix $M^{-1}$ such that
    \[ M^{-1} \times M = Id_n \]
  \end{block}

  \begin{block}{Properties of inverse matrices.}
    \begin{itemize}
    \item $P \times M = Id_n$ iff $M \times P = Id_n$.  
    \item when it exists, the inverse is unique: if $P \times M = Id_n$ and $Q \times M = Id_n$ then $P = Q$.
    \item $(M^{-1})^{-1} = M$
    \item If $M$ and $N$ are both invertible, then so is $M \times N$ and its inverse is $N^{-1}\times M^{-1}$.
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}{Proof of $P \times M = Id_n$ iff $M \times P = Id_n$}
  \begin{itemize}
  \item Assume $P\times M = Id_n$.
  \item Show first that $\widehat{M}$ is injective: if $M \times X = 0$ then $\underbrace{P \times M}_{Id_n} \times X = P \times 0 = 0$. Therefore $X = 0$. Hence $Ker(\widehat{M}) = \{0\}$, $M$ is injective.
  \item Since $\widehat{M}$ is a mapping between spaces of same dimension ($n$), $\widehat{M}$ is also surjective.
  \item Since $\widehat{M}$ is surjective,  for any $X \in \mathbb{R}^n$ we can find a $Y \in \mathbb{R}^n$ sucht that $X = M Y$.
  \item $(M\times P - Id_n ) \times X =  (M \times P - Id_n) \times M \times Y = M \times \underbrace{P \times M}_{Id_n} \times Y - M \times Y = 0$.
  \item So $\widehat{(M\times P - Id_n )}$ is the null function $X \mapsto 0$. By unicity of its matrix representation we then have $M\times P - Id_n =  0$.
  \item And finally, $M \times P = Id_n$.
  \end{itemize}
\end{frame}

\begin{frame}{Invertible matrix and isomorphisms.}
  \begin{block}{Proposition}
    $M$ is invertible iff $\widehat M$ is an automorphism of $\mathbb{R}^n$  (reminder: automorphism is an isomorphism between a vector space and itself).
  \end{block}

  \begin{exampleblock}{Proof}
    We know that $\widehat M$ is linear. We have to show that it is bijective. It suffices to show that it admits an inverse function.
    \begin{itemize}
    \item $M$ is invertible iff $\exists M^{-1}\quad M^{-1} \times M = Id_n$
    \item iff $\exists M^{-1} \quad \widehat{M^{-1} \times M} = \widehat{Id_n}$
    \item iff $\exists M^{-1} \quad \widehat{M^{-1}} \circ \widehat{M} = id_{\mathbb{R}^n}$
    \item iff $\exists f = M^{-1} \quad f \circ \widehat{M} = id_{\mathbb{R}^n}$
    \item iff $\widehat{M}$ bijective.
    \end{itemize}
  \end{exampleblock}

\end{frame}

\begin{frame}
  \begin{block}{Corolloray}
    \begin{itemize}
    \item If a linear map between finite dimenional spaces $f: V \mapsto W$ is an isomorphism then for any base $\mathcal{B}$ of $V$ and $\mathcal{B}'$ of $W$, $f \restriction_{\mathcal{B}, \mathcal{B}'}$ is invertible.
    \item If there exists some basis $\mathcal{B}$ of $V$ and $\mathcal{B}'$ of $W$ such that $f \restriction_{\mathcal{B}, \mathcal{B}'}$ is invertible, then $f$ is an isomorphism.
    \end{itemize}
  \end{block}

  \begin{exampleblock}{Proof}
    \begin{itemize}
    \item Assume $f$ is an isomorphism. We must have $dim(V) = dim(W)$, and we let $n = dim(V) = dim(W)$. Then $X \mapsto f(\langle X \rangle_{\mathcal{B}}) \restriction_{\mathcal{B}'}$ is an automorphism of $\mathbb{R}^{n}$ because it is a composition of isomorphisms. But $X \mapsto f(\langle X \rangle_{\mathcal{B}}) \restriction_{\mathcal{B}'} = \widehat{f \restriction_{\mathcal{B}, \mathcal{B}'}}$. By the preivous slide's proposition $f \restriction_{\mathcal{B}, \mathcal{B}'}$ is an isomomorphism.
    \item Assume there are $\mathcal{B}$ and $\mathcal{B}'$ such that $f \restriction_{\mathcal{B}, \mathcal{B}'}$. We proceed similarilly in the opposite direction: $\widehat{f \restriction_{\mathcal{B}, \mathcal{B}'}}$ is an isomorphism. By composition of isomorphisms, so is $f = v \mapsto  \langle f \restriction_{\mathcal{B}, \mathcal{B}'} \times (v \restriction_{\mathcal{B}}) \rangle_{\mathcal{B'}}$.
    \end{itemize}
  \end{exampleblock}
\end{frame}


\begin{frame}{Rank of a matrix}
  Given a matrix: \[A = \begin{array}{|ccc|} a_{1,1} & \dots & a_{1,m}\\ \vdots & & \vdots\\ a_{n,1} & \dots & a_{n, m} \end{array}\]
  \begin{itemize}
  \item The $j^{th}$ column of $A$ is the vector $(a_{1,j}, \dots, a_{n, j}) \in \mathbb{R}^n$.
  \item We write $A^{(j)}$ the $j^th$ column of $A$.
  \item The $i^{th}$ row of $A$ is the vector $(a_{i, 1}, \dots, a_{i, m}) \in \mathbb{R}^m$.
  \item We write $A[i]$ the $i^{th}$ row of $A$.
  \end{itemize}
\end{frame}


\begin{frame}{Rank of a matrix}
  \[A = \begin{array}{|ccc|} a_{1,1} & \dots & a_{1,m}\\ \vdots & & \vdots\\ a_{n,1} & \dots & a_{n, m} \end{array}\]
  
  \begin{block}{Definition}
    The rank of $A$ is the dimension of the space generated by the columns:
    \[rk(A) = dim(\mathcal{L}(A^{(1)}, \dots, A^{(m)}))\]
  \end{block}
  
\end{frame}

\begin{frame}
  \begin{block}{Equivalent definition}
    \[ rk(A) = dim(Im(\widehat{A}))\]
  \end{block}
\end{frame}

\begin{frame}
  \begin{block}{Proposition}
    \begin{itemize}
    \item An $(n,n)$ matrix $M$ is invertible iff $rk(M) = n$.
    \item If $P$ is an $(n,n)$ invertible matrix, $rk(P \times M) = rk(M \times P) = rk(M)$
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}{Proof}
    $rk(P \times M) = dim(Im(\widehat{P \times M})) = dim(Im(\widehat{P} \circ \widehat{M}))$. We now apply the rank theorem twice. One time to $\widehat{M}: \mathbb{R}^n \mapsto \mathbb{R}^n$ and another to $\widehat{P} \circ \widehat{M}: \mathbb{R}^n \mapsto \mathbb{R}^n$ so we have the two equalities:
    \[\begin{aligned}
    &n = \underbrace{dim(Im(\widehat{M}))}_{rk(M)} + dim(Ker(\widehat{M}))\\
    &n = \underbrace{dim(Im(\widehat{P} \circ \widehat{M}))}_{rk(P \times M)} + dim(Ker(\widehat{P} \circ \widehat{M}))
    \end{aligned}
    \]
    \[ rk(P \times M) = rk(M) + dim(Ker(\widehat{M})) - dim(Ker(\widehat{P} \circ \widehat{M})) \]
  \end{frame}

  \begin{frame}{Proof -- continued}
    \begin{equation} rk(P \times M) = rk(M) + dim(Ker(\widehat{M})) - dim(Ker(\widehat{P} \circ \widehat{M})) \end{equation}
    
    Now $X \in Ker(\widehat{P} \circ \widehat{M})$ iff $\widehat{P} \circ \widehat{M}(X) = 0$ iff $\widehat{P}(\widehat{M}(X)) = 0$ iff $\widehat{M}(X) \in Ker(\widehat{P})$. But $P$ is invertible so $\widehat{P}$ is an isomorphism, and so $Ker(\widehat{P}) = \{0\}$. Hence $X \in Ker(\widehat{P} \circ \widehat{M})$ iff $\widehat{M}(X) = 0$ \emph{i.e.} $X \in Ker(M)$. So $Ker(\widehat{P} \circ \widehat{M}) = Ker(\widehat{M})$.

    Replacing in equation 1), we get $rk(P \times M) = rank(M)$.

    Applying the result we just got to $A = M \times P$ and $Q = P$ we get
    \[rk(P \times M \times P) = rk(M \times P) = rk(M) \]
    Then we apply the result again to $B = P \times M \times P$ and $R = P^{-1}$ and get:
    \[rk(M \times P) = rk(\underbrace{P^{-1}}_{R} \times \underbrace{P \times  M \times P}_{B}) = rk(M \times P) = rk(M) \]
  
  
  \end{frame}

  \begin{frame}{Operations on row.}
    
    \begin{block}{Exercise}
      What is the result $P \times A$ for each of the following $P$:
      \[
      \only<1>{
        \underbrace{
          \begin{pmatrix}
            1 & 0 & \dots & \dots & \dots & \dots & \dots & \dots & \dots & 0 \\
            0 & \ddots & \ddots &  &  & & & & & \vdots \\
            \vdots & \ddots & \ddots & \ddots &    &   &  &  &  & \vdots\\
            \vdots &   & \ddots & \ddots & \ddots   &  &   &   &   & \vdots\\
            \vdots &  &  & \ddots & 1 & \ddots &  &   &  & \vdots\\
            \vdots &  & &  & \ddots & \alpha & \ddots &  & & \vdots\\
            \vdots & &  &  & & \ddots & 1 & \ddots & & \vdots\\
            \vdots &   &   &   &   &   & \ddots & \ddots & \ddots   & \vdots\\
            \vdots &   &   &  &   &    &  & \ddots  & \ddots & 0\\
            0 & \dots & \dots & \dots & \dots & \dots & \dots & \dots  & 0 & 1\\
      \end{pmatrix}}_{P = mul(\alpha, i)} \times \underbrace{\begin{pmatrix} a_{1,1} & \dots & a_{1,m}\\ \vdots & & \vdots\\ a_{n,1} & \dots & a_{n, m} \end{pmatrix}}_{A}}
      \only<2>{
        \underbrace{
        \begin{pmatrix}
            1 & 0 & \dots & \dots & \dots & \dots & \dots & \dots & \dots & 0 \\
            0 & \ddots & \ddots &  &  & & & & & \vdots \\
            \vdots & \ddots & 0 & \ddots &    & 1  &  &  &  & \vdots\\
            \vdots &   & \ddots & \ddots & \ddots   &  &   &   &   & \vdots\\
            \vdots &  &  & \ddots & 1 & \ddots &  &   &  & \vdots\\
            \vdots &  & 1  &  & \ddots & 0 &  \ddots &  & & \vdots\\
            \vdots & &  &  & & \ddots & 1 & \ddots & & \vdots\\
            \vdots &   &   &   &   &   & \ddots & \ddots & \ddots   & \vdots\\
            \vdots &   &   &  &   &    &  & \ddots  & \ddots & 0\\
            0 & \dots & \dots & \dots & \dots & \dots & \dots & \dots  & 0 & 1\\
      \end{pmatrix}}_{P = Exch(i_1, i_2)} \times \underbrace{\begin{pmatrix} a_{1,1} & \dots & a_{1,m}\\ \vdots & & \vdots\\ a_{n,1} & \dots & a_{n, m} \end{pmatrix}}_{A}
      }
      \only<3>{
        \underbrace{
          \begin{pmatrix}
            1 & 0 & \dots & \dots & \dots & \dots & \dots & \dots & \dots & 0 \\
            0 & \ddots & \ddots &  &  & & & & & \vdots \\
            \vdots & \ddots & \ddots & \ddots &    &   &  &  &  & \vdots\\
            \vdots &   & \ddots & \ddots & \ddots   &  &   &   &   & \vdots\\
            \vdots &  &  & \ddots & 1 & \ddots &  &   &  & \vdots\\
            \vdots &  & 1 &  & \ddots & 1 & \ddots &  & & \vdots\\
            \vdots & &  &  & & \ddots & 1 & \ddots & & \vdots\\
            \vdots &   &   &   &   &   & \ddots & \ddots & \ddots   & \vdots\\
            \vdots &   &   &  &   &    &  & \ddots  & \ddots & 0\\
            0 & \dots & \dots & \dots & \dots & \dots & \dots & \dots  & 0 & 1\\
      \end{pmatrix}}_{P = Add(i_1, i_2)} \times \underbrace{\begin{pmatrix} a_{1,1} & \dots & a_{1,m}\\ \vdots & & \vdots\\ a_{n,1} & \dots & a_{n, m} \end{pmatrix}}_{A}
}
      \]
    \end{block}
    
  \end{frame}


  \begin{frame}{Gauss elimination and matrices.}
    \begin{itemize}
    \item Think of entries in matrix $A$ as coefficient of an homogeneous linear system $A \times X = 0$.
    \item Each row operation used in the Gauss algorithm corresponds to multiplication by a matrix $P$ on the left.
    \item $A \times X = 0 \Rightarrow (P \times A) \times X = 0$.
    \item Moreover, the matrices $P$ are invertible (exercise).
    \item $(P \times A) \times X = 0 \Rightarrow P^{-1} \times (P \times A) \times X = 0 \Leftrightarrow A \times X = 0$. 
    \end{itemize}
  \end{frame}
  
  \begin{frame}
    \begin{block}{Gauss elimination on matrices:}
      Given an $n,m$ matrix $A$, one can use Gauss elimination to find a finite sequence of invertible matrices $P_1, \dots, P_k$ such that \[P_1 \times \dots \times P_k \times A = B \textnormal{ where } B \textnormal{ is in reduced echelon form}\]
    \end{block}

    \begin{exampleblock}{Reminder}
    \[ \begin{pmatrix}{}
      {\color{red}1}     & a_{1,2}    &  0     & 0   & \dots \\
      0     & 0           & {\color{red}1} &  0       & \dots\\
      0     & 0          &   0   & {\color{red}1} & \dots \\
      \vdots & \vdots     & \vdots      &  \vdots     & \dots\\
      0     &   0          &  0    & 0  & \dots \\
    \end{pmatrix}
    \]
    \end{exampleblock}
  \end{frame}

  \begin{frame}{Two cases}
    \[P_1 \times \dots \times P_k \times A = B \textnormal{ where } B \textnormal{ is in reduced echelon form}\]
    \begin{enumerate}
    \item $B$ has $l \ge 1$ lines with only $0$s.
      \begin{itemize}
      \item Then we know that the solutions of $B \times X = 0$ forms a vector space of dim $l$.
      \item But this space is also $Ker(\widehat{B}) = Ker(\widehat{A})$ so $A$ is non injective hence non invertible.
      \item Moreover if $A$ is an $(n,m)$ matrix, by the rank theorem, we have $rk(A) = m - l$.
      \end{itemize}
    \item $B$ has no line with only $0$s. We must have $B = Id_n$, so $A$ must be an $(n,n)$ matrix.
      \begin{itemize}
      \item $(P_1 \times \dots \times  P_k) \times A = Id_n$
      \item The inverse of $A$ is $(P_1 \times \dots \times  P_k)$
      \end{itemize}
    \end{enumerate}
      
  \end{frame}
  
\end{document}


